{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Titanic Dataset Big Data Pipeline\n",
        "This notebook demonstrates a full big data pipeline using PySpark for the Titanic dataset, including data preprocessing, visualization, and training a classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "24/12/12 14:29:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------+--------------------+------+------+-----+-----+------+--------+-------+--------+\n",
            "|pclass|survival|                name|   sex|   age|sibsp|parch|ticket|    fare|  cabin|embarked|\n",
            "+------+--------+--------------------+------+------+-----+-----+------+--------+-------+--------+\n",
            "|     1|       1|Allen, Miss. Elis...|female|  29.0|    0|    0| 24160|211.3375|     B5|       S|\n",
            "|     1|       1|Allison, Master. ...|  male|0.9167|    1|    2|113781|  151.55|C22 C26|       S|\n",
            "|     1|       0|Allison, Miss. He...|female|   2.0|    1|    2|113781|  151.55|C22 C26|       S|\n",
            "|     1|       0|Allison, Mr. Huds...|  male|  30.0|    1|    2|113781|  151.55|C22 C26|       S|\n",
            "|     1|       0|Allison, Mrs. Hud...|female|  25.0|    1|    2|113781|  151.55|C22 C26|       S|\n",
            "+------+--------+--------------------+------+------+-----+-----+------+--------+-------+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"TitanicPipeline\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load the Titanic dataset\n",
        "df = spark.read.csv(\"titanic.csv\", header=True, inferSchema=True)\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the stages of my pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Handle missing values\n",
        "df = df.fillna({\"age\": df.select(\"age\").dropna().rdd.map(lambda x: x[0]).mean()})\n",
        "df = df.dropna(subset=[\"Embarked\"])\n",
        "\n",
        "# Feature Engineering\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "\n",
        "# Stage 1\n",
        "# Replacing the string values in the column `sex` with numercial values\n",
        "sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"sexIndex\")\n",
        "\n",
        "# Stage 2\n",
        "# Replacing the string values in the column `embarked`` with numercial values\n",
        "embarked_indexer = StringIndexer(inputCol=\"embarked\", outputCol=\"embarkedIndex\")\n",
        "\n",
        "# Stage 3\n",
        "# Selecting the features\n",
        "assembler = VectorAssembler(inputCols=[\"pclass\", \"age\", \"fare\", \"sexIndex\", \"embarkedIndex\"],\n",
        "                            outputCol=\"features\", handleInvalid=\"skip\") # avoid handleInvalid=\"skip\". It is prefered to have a look at your data and decide how to handle ALL the missing values\n",
        "\n",
        "# Stage 4\n",
        "# Train Logistic Regression Model\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"survival\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline Integration\n",
        "Combine all stages into a PySpark pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/12/12 14:30:20 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
            "24/12/12 14:30:20 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Split the dataset\n",
        "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "pipeline = Pipeline(stages=[sex_indexer, embarked_indexer, assembler, lr])\n",
        "pipeline_model = pipeline.fit(train)\n",
        "pipeline_predictions = pipeline_model.transform(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AUC: 0.751700398142004\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "predictions = pipeline_model.transform(test)\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"survival\", rawPredictionCol=\"prediction\")\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"AUC: {auc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the Model and Results\n",
        "Save the trained model and prediction results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the pipeline model\n",
        "pipeline_model.write().overwrite().save(\"titanic_model\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
